project grading guidelines
1. submission and compilation
2. command line input (detect zero/ negative input)
3. code readability (comments and structure)
4. fork usage
5. initial shell and display format (messy display)
6. other command execution
7. error message display
8. history command execution (if !! or !N not in histry)
9. !! command execution
10. !N command execution

assignment grading guidelines
1. submission and compilation
2. command line input (detect zero/ negative input)
3. code readability (comments and structure)
4. fork usage
5. correct results
* Chapter 1
- program counter specifies location of next instruction to execute
- each thread requires one program counter
  
- cache coherency :: in multiprocessor environments all CPUs must have the most recent value in their cache
- spooling :: the overlapping of output of one job with input of another
* Chapter 2
Operating System Services:
- User Interface
- Program execution
- I/O operations
- File system manipulation
- communications
- error detection
- resource allocation
- accounting
- protection and security
Types of System Calls:
- Process Control
- File management
- Device management
- information maintenance
- communications
  - message passing model
    - create and delete communication connections
    - send and receives messages across these connections to specified host name or proess name
    - i.e. from client to server
  - shared-memory model
    - create and gain access to memory regions
- Protection
* Chapter 3
Process is divided into multiple parts
- text section :: program code
- data section :: containing global variables
- heap :: containing memory dynamically allocated during running time
- stack :: containing temporary data

** PCB
Process Control Block :: contains information associated with each process
most relevantly it contains
  - process state
  - program counter
  - cpu registers
additionally it contains
  - cpu scheduling information
  - memory management information
  - accounting information
  - i/o status information
 
- information must be stored and reloaded from pcb to allow cpu to switch from process to process
- this process is called a *context switch*
** Threads
- like a portion of a process
- requires its own program counter
** Process Scheduling
Maintains scheduling queues of processes:
  - job queue :: set of all processes in the system
  - ready queue 
  - device queues :: set of processes waiting for an i/o device
- Short-term scheduler or CPU scheduler :: selects which process should be executed next and allocates cpu 
  - since it is invoked frequently in must be fast
- Long-term scheduler or job scheduler :: selects which processes should be brought into the ready queue
  - controls the degree of multiprogramming
Processes can be:
  - i/o-bound process
  - cpu-bound process
  - the long-term scheduler strives for good process mix
- Medium-term scheduler :: remove process from memory to store on disk and bring back to memory
  - can be added if degree of multiprogramming needs to be decreased
** Process Creation
- process tree created by parent processes creating child processes
- child processes can have variable amount of dependence on parent process regarding:
  - execution
  - resource sharing
- child process has its own address space with a copy of the parent's image
** Process Termination
Execution ends normally followed by exit() system call:
- Resources de-allocated
- Returns status flag from child to parent
- PCB remains
Execution terminated by abort() system call:
- cascading termination of children's children
- parent may wait for termination of child process with wait()
  - pid = wait(&status)
- zombie process :: process waiting for parent to call wait()
- orphan :: parent terminated without calling wait()
  - init will take over and call wait()
** Inter-process Communication
- Independent process :: cannot affect or be affected by execution of another process
- Cooperating process :: can affect or be affected by execution of another process
- reasons for cooperating processes:
  - information sharing
  - computation speedup
  - modularity
  - convenience
- There are two models of IPC 
  - message passing
    - 
  - shared memory
*** Producer-Consumer Problem
- paradigm for cooperating processes
- exchange information via buffer
  - may be bounded or unbounded
*** Shared Memory
- tends to be the faster solution
- communication is under control of the user processes not the operating system
- major issue of allowing user processes to synchronize their actions on shared memory
  - e.g. not allowing both processes to write to the same address
*** Message Passing
- slower because of overhead of system calls
- easier to implement
- ideal for smaller amounts of data
- many choices to be made in specific implementation details (see textbook)
**** Synchronization
- Message passing may be either *blocking* or *non-blocking*
  - blocking (synchronous)
    - blocking send :: sender is blocked until message received
    - blocking receive :: receiver is blocked until message available
  - non-blocking (asynchronous)
    - non-blocking send :: sender sends message and continues
    - non-blocking receive :: receives gets either valid message or null message
**** Direct Communication
- connection established between exactly two processes
**** Indirect Communication
- messages are directed and received from "mailboxes" (aka ports)
- each mailbox has a unique ideal
- mailbox must be shared
- problems arise when mailbox shared with more than two processes
**** Buffering
- zero capacity :: no messages are queued on a link
  - sender always waits for receiver (*rendezvous*)
- bounded capacity :: finite length of n messages
  - sender only waits if link full
- unbounded capacity :: infinite length
  - sender never waits
*** Examples - Windows
- windows is message passing centric via *advanced local procedure call
- connection ports and communication ports are created to allows processes to communicate
- different message-passing techniques are used depending on the size of the message
  - smaller messages use a message queue
  - large messages passed through shared memory called a *section object*
  - larger messages that can not fit in the section object use an API call that allows server to write directly to the address space of the client
** Communication in client-server systems
*** Sockets
- socket :: an endpoint for communication
- combination of an ip address and port
- ports below 1024 are well known, for standard services
- two types of sockets:
  - connection-oriented (TCP)
  - connectionless (UDP)
*** Remote Procedure Calls
- sockets are a relatively low level method of communication between processes
- Client invokes a procedure on a remote host as if it were a local procedure
  - ports still used for service differentiation
  - but packets are not used
- stubs :: client-side proxy for the actual process on server
- client-side stub locates the server and *marshals* the parameters
  - to resolve differences between the systems (e.g. client little-endian and server big-endian)
  - XDR is used :: external data representation
- server-side stub receives, unpacks the marshaled parameters and performs procedure
Handling failure
- the at most once semantic is generally used, but the exactly once semantic may be used here
- the server can use at most once, but also send an ACK once the call has been executed
- the client uses exactly once, and continues sending the call until an ACK is received
Establishing Connection
- OS typically provides a rendezvous or *matchmaker* daemon to connect client and server
- alternatively a fixed-point address may be used
* Chapter 4
- thread :: a basic unit of cpu utilization
  - composed of :
    - thread ID
    - program counter
    - a register set
    - a stack
  - additionally multiple threads share:
    - a code section
    - a data section
    - and other OS resources
- When are threads useful?
  - when an application has multiple tasks which may need to wait for I/O
  - e.g. an application may implement threads for:
    - updating display
    - fetching data
    - spell checking
    - answering a network request
- why not just create new processes?
  - this incurs additional overhead which is unnecessary in many cases
- Benefits of multi-threading can be broken into four categories
  - responsiveness
  - resource sharing
  - economy :: reduced overhead vs process creation
  - scalability :: a single thread can run on only a single core
** multi-core programming
- parallelism :: ability for a system to perform more than one task simultaneously
  - i.e. multiple tasks are running at the same time
- concurrency :: ability for a system to allow more than one task to make progress
  - i.e. multiple tasks are running in the same window of time
- types of parallelism 
  - data parallelism :: distributes subsets of the same data across multiple cores, each running a portion of the same operation
  - task parallelism :: distributes threads across cores, each thread performing unique operation
- user threads :: management done by user-level thread library
- kernel threads :: managed by the kernel
*** Multi-threading models
- many-to-one model
  - many user-level threads mapped to a single kernel thread
  - entire process can be blocked by a single blocking system call
- one-to-one model
  - creating a user-level thread creates a kernel thread
  - drawback is the overhead of creating kernel threads
  - have to be careful about creating too many user-threads 
- many-to-many model
  - allows many user levels threads to be mapped to many kernel threads
  - allows for creation of many user-threads without worry and supports concurrency
*** Explicit Threading (using Thread Library)
- two primary ways of implementing:
  - library entirely in user space
  - kernel library entirely in OS
- pthreads :: posix standard API for thread creation and synchronization
  - all follow the same specification not necessarily the same implementation
*** Implicit Threading
- creation and management of threads done by compiler rather than programmer
- three methods explored:
  - Thread Pools
    - create a number of threads in a pool where they wait for use
    - slightly faster than creating a new thread
    - can limit number of threads by setting pool size
  - OpenMP
  - Grand Central Dispatch
** Threading Issues
- fork() and exec()
- does fork() duplicate only the calling thread or all threads
  - some UNIXes have two versions of fork
- exec() usually works by replacing the running process including all threads
** Signal Handling
1. signal is generated by particular event
2. signal is delivered to a process
3. signal must be handled by either a default or user-defined signal handler (which overrides default)

- synchronous signal :: generated by/delivered to the same running process
- asynchronous signal :: generated by an external event delivered to another process
- for single threaded process, signal delivery is straightforward
- for multi-threaded processes the question of which threads to deliver signal to arises
** Thread Cancellation
- target thread :: thread to be cancelled
- two general approaches:
  - asynchronous cancellation :: terminates the target thread immediately
  - deferred cancellation :: allows target thread to periodically check if it should be cancelled
    - termination occurs when a cancellation point is reached
    - then a cleanup handler is invoked
- Thread-local storage :: allows each thread to have its own copy of data
** Lightweight Process
- lightweight process :: allow communication between kernel and thread library
  - each LWP is attached to a single kernel thread
  - more like an intermediate data structure than a process
- application schedules user threads to available LWP instead of kernel threads
- scheduler activation provided upcalls - communication from kernel to upcall handler in thread library
  - e.g. kernel informs a thread will be blocked, and allocate a new LWP
  - then application can schedule another thread to the new LWP
* Chapter 5 - Synchronization
** Background
- processes can execute concurrently which can cause data inconsistency problems
- if both processes modify the same value, *race condition* problems may arise
** Critical-Section Problem
- consider a system of n processes
  - each process has a critical section such that only one process may be in critical section concurrently
    - otherwise data discrepancy may arise
- solution :: each section must ask permission to enter critical section
  - entry section :: where permission is requested
  - critical section
  - exit section
  - remainder section :: optional
- two approaches to critical-section handling:
  - preemptive :: a process in kernel mode could be preempted by another process
    - more responsive
    - don't have to worry about a program taking too long
  - non-preemptive :: a process in kernel mode can not be preempted by another processes
    - free from race conditions
    - only one process may be in kernel mode at a time
*** SOLUTION REQUIREMENTS:
  - mutual exclusion :: only one process may be in its critical section at a time
  - progress :: if no process is in its critical section, we must eventually select the next process to enter critical section
  - bounded waiting :: there must be some bound on how many items may enter their critical section after a process has requested to enter its critical section
** Peterson's Solution
- software solution to CSP
  - not guaranteed to work on modern architecture
** Synchronization Hardware      
- see problems 5.8 and 5.9
- locking :: protecting critical regions through the use of locks
- atomic :: non-interruptible unit
- for multiprocessor systems, it is inefficient to disable interrupts on all processors
  - instead we use atomic instructions:
    - test_and_set()
    - compare_and_swap()
  - see textbook, works through implementations that satisfy CSP requirements
** Mutex Locks
- previous solutions are quite complicated
- mutex is a much simpler solution
  - boolean variable indicates if lock is available or not
  - acquire() and release() system calls
  - calls to acquire and release must be atomic
- problem with this solution: it introduces busy waiting
  - busy waiting :: while a process is in its critical section, any other process attempting to enter critical section stuck on acquire()
  - called a spinlock
** Semaphore
- may be used to solve various synchronization problems
- an integer variable that can only be accessed through two standard atomic functions:
  - wait() :: waits while S is 0 then decrements and finishes after
  - signal() :: increments S
- if S is a binary value, then we have a mutex lock
- if S is an integer value, then we have a *Counting Semaphore*
*** Problems
- deadlock :: multiple processes waiting indefinitely for an event that can be caused by on the waiting processes
- starvation :: indefinite blocking 
  - e.g. if we use a stack to feed our implementation
- priority inversion ::
*** Bounded-Buffer Problem
- semaphore mutex init to 0
- semaphore full init to 0 :: number of buffers containing an item
- semaphore empty init to n :: number of empty buffers
*** Readers-Writers Problem
- Problem:
  - allow multiple readers simultaneously
  - only one singer writer can access the shared data at the same time
- Variations:
  - no readers kept waiting unless writer has permission to use shared object
  - once writer is ready, it performs the write ASAP
Both variations have possible starvation problems that must be addressed

- rw_mutex :: semaphore initialized to 1
- mutex :: semaphore initialized to 1
- read_count :: integer initialized to 0

- mutex ensures mutual exclusion semaphore when read_count is update
- read_count keeps track of how many processes are currently reading
- rw_mutex functions as a mutual exclusion semaphore for writers
*** Dining-Philosophers Problem
- Problem:
  - philosophers alternate eating and thinking
  - to eat the philosopher needs two chopsticks
  - they each share one chopstick with their left neighbor and one with their right neighbor
  - they can only try to pick up one chopstick at a time
- see textbook for more
** Monitors
- a high-level abstraction that helps with process synchronization
- we have conditional variables and queues for processes waiting to enter critical section
** Semaphore implementation with no busy waiting  
- two operations
  - block :: place the process invoking the operation on the appropriate waiting queue
  - wakeup :: remove one of processes in the waiting queue and place in ready queue
basically wait() and signal() end up being calls to block() and wakeup() respectively
** Monitor implementation using semaphores
- semaphore mutex
- semaphore next
- int next_count
* Chapter 7
** System Model
- each process utilizes a resource as follows:
  - request
  - use
  - release
** Deadlock characterization
- mutual exclusion :: only one process can use a resource ata time
- hold and wait :: a process holding a resource is waiting to acquire additional resources held by other processors
- no preemption :: a resource can be released only voluntarily by the process holding it
- circular wait :: there's a set of resource in which a waits on b that waits on c that waits on d that waits on a
** Resource-allocation graph 
- V is partitioned into two types
  - P :: processes
  - R :: resources
- E is partitioned into two types
  - request edge :: directed edge P -> R
  - assignment edge :: directed edge R -> P
* Chapter 6
- successful scheduling involves utilizing downtime from things like blocking system calls
- cpu burst :: time allocated to cpu
- cpu burst :: time allocated to i/o
 
- short-term scheduler :: selects process in ready queue and allocates CPU
- preemptive / non-preemptive
 
  - dispatcher module :: gives control of cpu to process selected by short-term scheduler
  - *dispatch latency* :: time taken to stop one process and start another
** scheduling criteria
- cpu utilization
- throughput :: number of processes that complete their execution per time unit
- turnaround time :: amount of time to execute a particular processors (from arrival to finish)
- waiting time (in ready queue)
- response time (from request to first response)
** FCFS scheduling
- convoy effect :: short processes waiting for long processes
** SJF scheduling
- if we know the length of the next CPU burst for each process, SJF is optimal
- the problem is that we usually will not know the length of next CPU burst
- we can also create a preemptive variant : shortest remaining time first
- exponential average :: a technique for prediction next CPU burst duration:
    t_n = actual length of nth cpu burst
    tau_n+1 = predicted value for n+1th cpu burst
    alpha, 0 <= alpha <= 1
    define: tau_n+1 = alpha*t_n + (1-alpha) * tau_n
** Priority Scheduling
- a priority number is associated with each process
  - smallest int is highest priority
- CPU is allocated to process with the highest priority
- problem - *starvation* :: low priority processes may never execute
- solution - *aging* :: increase priority with time
** Round Robin
- Each process gets a small unit of CPU time (*time quantum* q)
- Timer interrupts after every quantum to schedule next process
*** round robin performance
- No process waits more than (n-1) * q time
- if q is large :: we essentially have FIFO
- if q is small :: q must be large w.r.t context switch or overhead will be high
- heuristic : 80% of cpu bursts should be shorter than q
** Multilevel Queue
- ready queue could be partitioned into separate queues e.g.
  - foreground (interactive)
  - background (batch)
- process permanently assigned to one of these queues
** Multilevel feedback queue
- multilevel feedback queue :: can go from one queue to another based on certain criteria
- each queue has its own scheduling algorithm:
  - foreground uses RR
  - background uses FCFS
- scheduling must be done between the queues
  - Fixed priority scheduling :: e.g. one queue always has priority (has possibility of starvation)
  - time slice :: each queue gets a certain amount of CPU time
- in another implementation we could move processes between queues
  - this is one way that we could implement aging
** Algorithmic Evaluation 
- analytic evaluation
  - deterministic modeling
  - queueing models
- simulation
- implementation
*** Deterministic Modeling
- type of analytic evaluation
- takes a particular predetermined workload and determines the performance of different algorithms
*** Queueing Models
- input is determined probabilistic-ally
  - e.g. arrival of processes, cpu burst, and i/o burst
  - most of these things follow exponential distribution
*** Simulation
- programmed model of computer system
- clock is a variable
- gather statistics indicating algorithm performance
- instead of using probabilistic-driven input we can use a trace tape
- trace tape :: input recorded from actual system that can be fed into simulation
*** Implementation
- implementing a new scheduler and testing in real systems is expensive and risky
- most flexible schedulers can be modified per-system
  - or provide APIs to modify priorities
  - but environments vary
** Thread Scheduling
- process-contention scope :: scheduling done within a process between threads
- system-contention scope :: scheduling done between all threads in a system
** Windows Scheduling
- 32-level priority scheme (higher number is higher priority)
- 1-15 for variable classes, 16-31 for real-time classes
- priority 0 for memory-management thread
- queue for each priority
- runs idle thread if no other run-able threads are available

- within each priority class there are 7 possible relative priority

** Dispatcher Shell
* Chapter 8
- CPU can only access main memory and registers
- Register access takes one clock, but main memory can take longer, *stall*
- *Cache* sits between register and main memory
** Base and Limit Registers
- a pair of addresses that define the beginning and size of a partition in memory
** Address Binding
- input queue :: bring program to disk for execution
- compile time :: bind symbolic addresses in source code to relocatable addresses
- load time :: bind relocatable addresses to physical addresses
- execution time :: bind dynamically loaded system libraries (*dynamic linking*)
- *Logical Addresses vs Physical Address*
** Memory-Management Unit
- Base register becomes relocation register
- 
** Dynamic Loading 
- Routine is not loading until it is called
- All routines kept on disk in relocatable load format
** Dynamic Linking 
- static linking :: system libraries and program code
- dynamic linking :: linking postponed until execution time
  - particularly useful for libraries
  - small piece of code called *stub* used to locate appropriate routine
  - also called *shared libraries*
** Swapping
- backing store :: hold copies of all memory images for all users
  - ready queue holds ready to run processes including those in backing store
  - unstructured data, allows for faster use than normal filesystem
- roll out, roll in :: variant of swapping used for priority based scheduling
  - lower-priority process swapped out for higher-priority process
** MISSING NOTES!
** Dynamic Storage-Allocation Problem
** Fragmentation
** Segmentation
** Paging
- divide physical memory into fixed-sized blocks called *frames*
- divide logical memory into same-sized blocks called *pages*
- *page table* allows translate logical to physical addresses
  - page table located in pcb
- paging eliminates external fragmentation
- internal fragmentation:
  - (space required) % (page size) = internal fragmentation for a particular process
- this implies that small page size is preferred, however each page requires some overhead,
so we will not want our pages to be too small.
- for one, our page table must grow with number of pages
- Typical size today is ~4K
*** Free frames
- operating system must store information about which frames are allocated/availalbe
- this information is stored in a data structure called a *frame table*
*** Implementation of Page Table 
- page table kept in main memory
- PTBR :: page-table base register
  - points to the page table
- PTLR :: page-table length register
  - indicates size of the page table
- when using page tables, every data/instruction access requires two memory accesses
  - one to find physical address from page table
  - another to access physical address
- how can we deal with this?
  - register to hold page table
  - use of special, fast hardware cache called *associate memory*
also known as *translation look-aside buffer* / *TLB*
**** Associative memory
- when we perform a page look up, we can load into TLB
  - this is called a *TLB miss* when we don't already have TLB entry
- next time we perform that page lookup, we can lookup from TLB, *TLB hit*
- TLB is typically small
  - replacement policies must be considered, 
  - some entries can be *wired down* for permanent fast access
  *Effective Access Time*
- associate lookup time: \epsilon time units
- memory access time: t time units
- hit ratio: \alpha
    Effective Access Time (EAT) =
      (1 * t + \epsilon) * \alpha + (2 * t + \epsilon)*(1 - \alpha)
**** Address-Space Protection
- some TLBs store *address-space identifier (ASID)* in each TLB entry,
- uniquely identifies each process to provide address-space protection
  - Not matching ASID is a TLB miss
  - allows for multiple processors
  - otherwise TLB would need to be flushed on every context switch
*** Memory Protections via Page Table
- valid-invalid bit attached to each entry in page table:
  - valid :: page is in process logical address space
  - invalid :: page not in process' logical address space
  - or use page-table length register
*** Shared Pages
shared code 
- one copy of read-only (*reentrant*) code shared among processess
private code and data
- each process keeps a separate copy of this
** Structure of Page Table
- For a 32-bit logical address space and 4KB page size, 
  - page table takes 4 MB
- How do we allocate this memory?
  - *hierarchical page table*
  - *hashed page table*
  - *inverted page table*
- A hierarchical page table with many pages is also known as
   *forward mapping page table*
*** Hashed Page Tables 
- common in address spaces greater than 32 bits
- virtual page number is hashed into a page table
- collisions are dealt with by chaining
- A variation for this scheme has been proposed called *clustered page tables*
  - similar to hashed page tables except:
  - each bucket refers to several pages instead of just 1
*** Inverted Page Table
- create a page table based on pid that translates to a physical address
* Chapter 9
- program needs to be in memory to execute, but entire program not needed at same time
- so why don't we partially-load programs?
- would mean programs aren't limited by physical memory constraint
- virtual memory :: separation of user logical memory from physical memory
- virtual address space :: logical view of how process is stored in memory
virtual memory can be implemented by:
  - demand paging
  - demand segmentation
- sparse address space means that we can allocate a large logical address space,
and we do not need to allocate physical memory for unused sections
- also system libraries may be shared via mapping into virtual address space
** Demand Paging
- can bring entire process into memory at load time
- or bring page into memory only when needed
  - less i/o needed
  - less memory needed
  - faster response
- in general free pages are allocated from a pool
- lazy swapper :: only swaps a page into memory when page is needed
- pager :: swapper that deals with pages
- demand paging requires mmu functionality of valid-invalid bit
  - valid :: memory resident
  - invalid :: not in memory
- if valid-invalid bit is invalid during address translation, a *page fault* occurs
*** Page Fault
two cases:
- invalid reference -> abort
- just not in memory:
  1. find free frame
  2. swap page into frame
  3. reset tables to indicate page now in memory
    - set validation bit to valid
  4. restart the instruction that caused page fault
*** Pure demand paging
- start with no pages in memory
- OS sets instruction pointer to first instruction of process -> page fault
- a single instruction could access multiple pages, trigger multiple page faults
  - programs tend to have *locality of reference* so performance is not like this worst case
Hardware required for demand paging:
- page table with validation bit
- secondary memory (swap space)
- instruction restart
*** Performance of Demand Paging
- Three major activities in page-fault service time:
  - service the interrupt
  - read the page - majority of time
  - restart the process
*** Demand Paging Optimizations
- swap space i/o faster than file system i/o
  - swap allocated in larger chunks than file system
possible methods:
- copy entire process image to swap space at load time then page in and out
- demand page program binary from disk, but discard rather than page out when done
*** Copy-on-write
- allows both parent and child process to initially share the same pages in memory
- page only copied when modified (so only modified pages ever need to be copied)
** What if there are no free frames?
- page replacement :: find page in memory, but not in use. Page it out
  - what should the process do? terminate? swap out? replace the page?
Page replacement requires:
- modifying page-fault service routine to include page replacement
*** Basic page replacement
1. lookup address of desired page on disk
2. Find a free frame
   - if there is no free frame, use a page replacement algorithm to select a victim frame
   - write victim frame to disk if dirty
3. bring the desired page into the newly free frame, update page and frame table
4. continue process
*** page and frame replacement algorithms
- evaluate algorithm using reference string, containing list of page number to reference
some algorithms:
1. FIFO 
2. optimal (not actually possible, uses knowledge of future)
3. Least recently used
   - can be implemented using a counter associated with each page entry
   - can be implemented using a stack
     - updating after page reference is faster
     - but don't have to search through list for replacement
**** LRU Approximation Algorithm
- since lru implementations are expensive, it is easier to approximate
- special hardware is needed:
  - reference bit :: associated with each page, initially 0 set to 1 on page reference
- this results in *second-chance algorithm*
  - FIFO, but with reference bit
  - if page to be replaced has bit = 1, set to 0 then replace next page instead
- this can be further enhanced by also having a modify/dirty bit
- prioritize pages with both bits = 0
** Allocation of Frames
- two major allocation schemes:
  - fixed allocation
    - equal allocation
    - proportional allocation :: proportional to size of process
  - priority allocation
** replacement   
- global replacement :: process can take a frame from another frame as replacement
- local replacement :: process can only select from its own allocated frames
