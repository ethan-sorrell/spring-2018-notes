project grading guidelines
1. submission and compilation
2. command line input (detect zero/ negative input)
3. code readability (comments and structure)
4. fork usage
5. initial shell and display format (messy display)
6. other command execution
7. error message display
8. history command execution (if !! or !N not in histry)
9. !! command execution
10. !N command execution

assignment grading guidelines
1. submission and compilation
2. command line input (detect zero/ negative input)
3. code readability (comments and structure)
4. fork usage
5. correct results
* Chapter 1
- program counter specifies location of next instruction to execute
- each thread requires one program counter
  
- cache coherency :: in multiprocessor environments all CPUs must have the most recent value in their cache
- spooling :: the overlapping of output of one job with input of another
* Chapter 2
Operating System Services:
- User Interface
- Program execution
- I/O operations
- File system manipulation
- communications
- error detection
- resource allocation
- accounting
- protection and security
Types of System Calls:
- Process Control
- File management
- Device management
- information maintenance
- communications
  - message passing model
    - create and delete communication connections
    - send and receives messages across these connections to specified host name or proess name
    - i.e. from client to server
  - shared-memory model
    - create and gain access to memory regions
- Protection
* Chapter 3
Process is divided into multiple parts
- text section :: program code
- data section :: containing global variables
- heap :: containing memory dynamically allocated during running time
- stack :: containing temporary data

** PCB
Process Control Block :: contains information associated with each process
most relevantly it contains
- process state
- program counter
- cpu registers
additionally it contains
- cpu scheduling information
- memory management information
- accounting information
- i/o status information
 
- information must be stored and reloaded from pcb to allow cpu to switch from process to process
- this process is called a *context switch*
** Threads
- like a portion of a process
- requires its own program counter
** Process Scheduling
Maintains scheduling queues of processes:
  - job queue :: set of all processes in the system
  - ready queue 
  - device queues :: set of processes waiting for an i/o device
- Short-term scheduler or CPU scheduler :: selects which process should be executed next and allocates cpu 
  - since it is invoked frequently in must be fast
- Long-term scheduler or job scheduler :: selects which processes should be brought into the ready queue
  - controls the degree of multiprogramming
Processes can be:
  - i/o-bound process
  - cpu-bound process
  - the long-term scheduler strives for good process mix
- Medium-term scheduler :: remove process from memory to store on disk and bring back to memory
  - can be added if degree of multiprogramming needs to be decreased
** Process Creation
- process tree created by parent processes creating child processes
- child processes can have variable amount of dependence on parent process regarding:
  - execution
  - resource sharing
- child process has its own address space with a copy of the parent's image
** Process Termination
Execution ends normally followed by exit() system call:
- Resources de-allocated
- Returns status flag from child to parent
- PCB remains
Execution terminated by abort() system call:
- cascading termination of children's children
- parent may wait for termination of child process with wait()
  - pid = wait(&status)
- zombie process :: process waiting for parent to call wait()
- orphan :: parent terminated without calling wait()
  - init will take over and call wait()
** Interprocess Communication
- Independent process :: cannot affect or be affected by execution of another process
- Cooperating process :: can affect or be affected by execution of another process
- reasons for cooperating processes:
  - information sharing
  - computation speedup
  - modularity
  - convenience
- There are two models of IPC
  - message passing
  - shared memory
*** Producer-Consumer Problem
- paradigm for cooperating processes
- exchange information via buffer
  - may be bounded or unbounded
*** Shared Memory
- tends to be the faster solution
- communication is under control of the user processes not the operating system
- major issue of allowing user processes to synchronize their actions on shared memory
  - e.g. not allowing both processes to write to the same address
*** Message Passing
- slower because of overhead of system calls
- easier to implement
- ideal for smaller amounts of data
- many choices to be made in specific implementation details (see textbook)
**** Synchronization
- Message passing may be either *blocking* or *non-blocking*
  - blocking (synchronous)
    - blocking send :: sender is blocked until message received
    - blocking receive :: receiver is blocked until message available
  - non-blocking (asynchronous)
    - non-blocking send :: sender sends message and continues
    - non-blocking receive :: receives gets either valid message or null message
**** Direct Communication
- connection established between exactly two processes
**** Indirect Communication
- messages are directed and received from "mailboxes" (aka ports)
- each mailbox has a unique ideal
- mailbox must be shared
- problems arise when mailbox shared with more than two processes
**** Buffering
- zero capacity :: no messages are queued on a link
  - sender always waits for receiver (*rendezvous*)
- bounded capacity :: finite length of n messages
  - sender only waits if link full
- unbounded capacity :: infinite length
  - sender never waits
*** Examples - Windows
- windows is message passing centric via *advanced local procedure call
- connection ports and communication ports are created to allows processes to communicate
- different message-passing techniques are used depending on the size of the message
  - smaller messages use a message queue
  - large messages passed through shared memory called a *section object*
  - larger messages that can not fit in the section object use an API call that allows server to write directly to the address space of the client
** Communication in client-server systems
*** Sockets
- socket :: an endpoint for communication
- combination of an ip address and port
- ports below 1024 are well known, for standard services
- two types of sockets:
  - connection-oriented (TCP)
  - connectionless (UDP)
*** Remote Procedure Calls
- sockets are a relatively low level method of communication between processes
- Client invokes a procedure on a remote host as if it were a local procedure
  - ports still used for service differentiation
  - but packets are not used
- stubs :: client-side proxy for the actual process on server
- client-side stub locates the server and *marshals* the parameters
  - to resolve differences between the systems (e.g. client little-endian and server big-endian)
  - XDR is used :: external data representation
- server-side stub receives, unpacks the marshaled parameters and performs procedure
Handling failure
- the at most once semantic is generally used, but the exactly once semantic may be used here
- the server can use at most once, but also send an ACK once the call has been executed
- the client uses exactly once, and continues sending the call until an ACK is received
Establishing Connection
- OS typically provides a rendezvous or *matchmaker* daemon to connect client and server
- alternatively a fixed-point address may be used
* Chapter 4
- thread :: a basic unit of cpu utilization
  - composed of :
    - thread ID
    - program counter
    - a register set
    - a stack
  - additionally multiple threads share:
    - a code section
    - a data section
    - and other OS resources
- When are threads useful?
  - when an application has multiple tasks which may need to wait for I/O
  - e.g. an application may implement threads for:
    - updating display
    - fetching data
    - spell checking
    - answering a network request
- why not just create new processes?
  - this incurs additional overhead which is unnecessary in many cases
- Benefits can be broken into four categories
  - responsiveness
  - resource sharing
  - economy :: reduced overhead vs process creation
  - scalability :: a single thread can run on only a single core
** multicore programming
- parallelism :: ability for a system to perform more than one task simultaneously
  - i.e. multiple tasks are running at the same time
- concurrency :: ability for a system to allow more than one task to make progress
  - i.e. multiple tasks are running in the same window of time
- types of parallelism 
  - data parallelism :: distributes subsets of the same data across multiple cores, each running a portion of the same operation
  - task parallelism :: distributes threads across cores, each thread performing unique operation
- user threads :: management done by user-level thread library
- kernel threads :: managed by the kernel
*** Multi-threading models
- many-to-one model
  - many user-level threads mapped to a single kernel thread
  - entire process can be blocked by a single blocking system call
- one-to-one model
  - creating a user-level thread creates a kernel thread
  - drawback is the overhead of creating kernel threads
  - have to be careful about creating too many user-threads 
- many-to-many model
  - allows many user levels threads to be mapped to many kernel threads
  - allows for creation of many user-threads without worry and supports concurrency
*** Thread Library
- two primary ways of implementing:
  - library entirely in user space
  - kernel library entirely in OS
- pthreads :: posix standard API for thread creation and synchronization
  - all follow the same specification not necessarily the same implementation
*** Implicit Threading
- creation and management of threads done by compiler rather than programmer
- three methods explored:
  - Thread Pools
    - create a number of threads in a pool where they wait for use
    - slightly faster than creating a new thread
    - can limit number of threads by setting pool size
  - OpenMP
  - Grand Central Dispatch
** Threading Issues
- fork() and exec()
- does fork() duplicate only the calling thread or all threads
  - some UNIXes have two versions of fork
- exec() usually works by replacing the running process including all threads
** Signal Handling
1. signal is generated by particular event
2. signal is delivered to a process
3. signal must be handled by either a default or user-defined signal handler (which overrides default)

- synchronous signal :: generated by/delivered to the same running process
- asynchronous signal :: generated by an external event delivered to another process
- for single threaded process, signal delivery is straightforward
- for multi-threaded proesses the question of which threads to deliver signal to arises
** Thread Cancellation
- target thread :: thread to be cancelled
- two general approaches:
  - asynchronous cancellation :: terminates the target thread immediately
  - deferred cancellation :: allows target thread to periodically check if it should be cancelled
    - termination occurs when a cancellation point is reached
    - then a cleanup handler is invoked
- Thread-local storage :: allows each thread to have its own copy of data
** Lightweight Process
- lightweight process :: allow communication between kernel and thread library
  - each LWP is attached to a single kernel thread
  - more like an intermediate data structure than a process
- application schedules user threads to available LWP instead of kernel threads
- scheduler activation provided upcalls - communication from kernel to upcall handler in thread library
  - e.g. kernel informs a thread will be blocked, and allocate a new LWP
  - then application can schedule another thread to the new LWP
* Chapter 5
** Background
- processes can execute concurrently which can cause data inconsistency problems
- if both processes modify the same value, *race condition* problems may arise
** Critical-Section Problem
- consider a system of n processes
  - each process has a critical section such that only one process may be in critical section concurrently
    - otherwise data discrepancy may arise
- solution :: each section must ask permission to enter critical section
  - entry section :: where permission is requested
  - critical section
  - exit section
  - remainder section :: optional
- two approaches to critical-section handling:
  - preemptive :: a process in kernel mode could be preempted by another process
    - more responsive
    - don't have to worry about a program taking too long
  - non-preemptive :: a process in kernel mode can not be preempted by another processes
    - free from race conditions
    - only one process may be in kernel mode at a time
*** *solution requirements*:
  - mutual exclusion :: only one process may be in its critical section at a time
  - progress :: if no process is in its critical section, we must eventually select the next process to enter critical section
  - bounded waiting :: there must be some bound on how many items may enter their critical section after a process has requested to enter its critical section
** Peterson's Solution
- software solution to CSP
  - not guaranteed to work on modern architecture
** Synchronization Hardware      
- see problems 5.8 and 5.9
- locking :: protecting critical regions through the use of locks
- atomic :: non-interruptible unit
- for multiprocessor systems, it is inefficient to disable interrupts on all processors
  - instead we use atomic instructions:
    - test_and_set()
    - compare_and_swap()
  - see textbook, works through implementations that satisfy CSP requirements
** Mutex Locks
- previous solutions are quite complicated
- mutex is a much simpler solution
  - boolean variable indicates if lock is available or not
  - acquire() and release() system calls
  - calls to acquire and release must be atomic
- problem with this solution: it introduces busy waiting
  - busy waiting :: while a process is in its critical section, any other process attempting to enter critical section stuck on acquire()
  - called a spinlock
** Semaphore
- may be used to solve various synchronization problems
- an integer variable that can only be accessed through two standard atomic functions:
  - wait() :: waits while S is 0 then decrements and finishes after
  - signal() :: increments S
- if S is a binary value, then we have a mutex lock
- if S is an integer value, then we have a *Counting Semaphore*
*** Problems
- deadlock :: multiple processes waiting indefinitely for an event that can be caused by on the waiting processes
- starvation :: indefinite blocking 
  - e.g. if we use a stack to feed our implementation
- priority inversion ::
*** Bounded-Buffer Problem
- semaphore mutex init to 0
- semaphore full init to 0 :: number of buffers containing an item
- semaphore empty init to n :: number of empty buffers
*** Readers-Writers Problem
- Problem:
  - allow multiple readers simultaneously
  - only one singer writer can access the shared data at the same time
- Variations:
  - no readers kept waiting unless writer has permission to use shared object
  - once writer is ready, it performs the write ASAP
Both variations have possible starvation problems that must be addressed

- rw_mutex :: semaphore initialized to 1
- mutex :: semaphore initialized to 1
- read_count :: integer initialized to 0

- mutex ensures mutual exclusion semaphore when read_count is update
- read_count keeps track of how many processes are currently reading
- rw_mutex functions as a mutual exclusion semaphore for writers
*** Dining-Philosophers Problem
- Problem:
  - philosophers alternate eating and thinking
  - to eat the philosopher needs two chopsticks
  - they each share one chopstick with their left neighbor and one with their right neighbor
  - they can only try to pick up one chopstick at a time
- see textbook for more
** Monitors
- a high-level abstraction that helps with process synchronization
- 
** Semaphore implementation with no busy waiting  
- two operations
  - block :: place the process invoking the operation on the appropriate waiting queue
  - wakeup :: remove one of processes in the waiting queue and place in ready queue
basically wait() and signal() end up being calls to block() and wakeup() respectively
** Monitor implementation using semaphores
- semaphore mutex
- semaphore next
- int next_count
* Chapter 7
** System Model
- each process utilizes a resource as follows:
  - request
  - use
  - release
** Deadlock characterization
- mutual exclusion :: only one process can use a resource ata time
- hold and wait :: a process holding a resource is waiting to acquire additional resources held by other processors
- no preemption :: a resource can be released only voluntarily by the process holding it
- circular wait :: there's a set of resource in which a waits on b that waits on c that waits on d that waits on a
** Resource-allocation graph 
- V is partitioned into two types
  - P :: processes
  - R :: resources
- E is partitioned into two types
  - request edge :: directed edge P -> R
  - assignment edge :: directed edge R -> P
